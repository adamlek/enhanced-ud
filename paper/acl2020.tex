%
% File acl2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\usepackage{subcaption}
\usepackage[capitalize]{cleveref}
\usepackage{tikz}
\usetikzlibrary{arrows,positioning,fit,shapes}
\usepackage{todonotes}
\usepackage{tikz-dependency}

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{How much of enhanced UD is contained in UD?}
% How enhanced is enhanced UD?

\author{Adam Ek \qquad Jean Philippe Bernardy\\
    Centre for Linguistic Theory and Studies in Probability \\
    Department of Philosophy, Linguistics and Theory of Science \\
    University of Gothenburg \\
    \texttt{\{adam.ek, jean-philippe.bernardy\}@gu.se} \\}

\date{}

\begin{document}
\maketitle
    
\begin{abstract}
In this paper, we present the submission of team CLASP to the IWPT
2020 Shared Task on parsing enhanced universal dependencies
\citep{EUDparsingST:2020}. We develop a tree-to-graph
transformation algorithm based on dependency patterns. This
algorithm can transform gold UD trees to EUD graphs with an ELAS
score of $81.55$ and a EULAS score of $96.70$.  These results show
that much of the information needed to construct EUD graphs from
UD trees are present in the UD trees.
%
Coupled with a standard UD parser, the method applies to the
official test data and yields and ELAS score of $67.85$ and a EULAS
score is $80.18$.
\end{abstract}

\section{Introduction}
% UD intro
Universal Dependencies (UD) is a syntactic annotation schema
focusing on representing shallow syntactic dependencies between
\emph{words}. One of the goals of UD has been to use it in
semantic downstream tasks, such as event extraction
\citep{fares20182018, mcclosky2011event} or negation resolution
\citep{fares20182018} among others. In general, UD can be used as
a shallow representation of argument structures, which are useful
in a wide array of semantic tasks.

However, the UD format restricts the shape of dependencies to a tree
structure.  This can be limiting because semantics dependencies can
in principle exhibit any graph structure.
% EUD intro
To remedy this situation, the Enhanced Universal Dependencies
(EUD) schema was proposed \citep{schuster2016enhanced}. The goal
of the schema is to make certain implicit dependencies explicit,
such as conjoined subjects and objects. The enhanced dependencies
include additional edges between words, as well as augmented
labels. For example in enhanced dependencies, the conjunction
relation also include what type of conjunction is used, e.g.
\textit{and, or, but} and so on.

% What we do in this paper
In this paper, we present our submission for the IWPT 2020 Shared
Task on parsing enhanced universal dependencies from annotated UD
treebanks. The task target treebanks from 17 different languages
where the majority of the languages are Indo-European with five
notable exceptions, Tamil (Dravidian), Arabic (Semitic), Finnish
and Estonian (Uralic).  The goal of the task to produce valid EUD
graphs, given raw text as input.

In this context, this paper proposes to test the following hypothesis:
\begin{enumerate}
\item[(H1)] Most of the information provided by the EUD schema is
  contained in the basic UD schema.
\end{enumerate}

That is, if (H1) holds, then it is possible to map, algorithmically,
UD trees to EUD graphs.

Thus, we set out to implement such an algorithm. Concretely, we construct  a
tree-to-graph transformation, which recognizes patterns in the (basic)
universal dependencies to derive enhanced dependencies.
%


This experiment provides a \emph{lower bound} on the amount of EUD
information which can be extracted from raw UD information, for
representative inputs.  In other words, we measure how much of EUD is
contained in UD. This becomes a lower bound, because a better algorithm
can always be conceived, and do better.

% JP: I have deleted this upper bound thing. It isn't a lowerbound,
% but really a baseline, which is addressed below.

Conversely, additionally, by running our algorithm after a
state-of-the-art basic UD parser, we will provide a baseline for the
EUD reconstruction task.
%
% General idea: as described by the website
%(https://universaldependencies.org/u/overview/enhanced-syntax.html)
%for many cases, enhanced UD is a function of UD.
%
% Shared task introduction
%Indeed, in the IWPT shared task, teams are asked to produce enhanced
%dependencies in 17 different languages. 

The enhanced dependency parsing is evaluated using two metrics,
ELAS and EULAS. ELAS calculate the $F_1$-score over both enhanced
arcs and labels (for example: ``and'' in the label
``conj:and''). EULAS on the other hand disregard label
enhancements and calculate the $F_1$-score over the enhanced
edges. To illustrate, given the gold label ``conj:and'' and a system
prediction of ``conj:or'' (or ``conj'', i.e. if nothing is
appended to the label), ELAS will count this as an error while
EULAS won't.

%The enhanced dependency parsing is evaluated using two metrics,
%ELAS and EULAS. ELAS calculates the $F_1$-score over all enhanced
%dependencies, the metric includes both edges and edge labels. A
%secondary metric, EULAS, is used which calculates the $F_1$-score
%over the edges only.

% AE-FINAL: Vindicated?
% results in two lines
Our results show that (H1) is vindicated: we achieve an ELAS score
of $81.55$ and EULAS of $96.70$ on human-annotated dependency
trees.  As a baseline for the shared task, our method is also
effective, achieving an ELAS score of $67.85$ and a EULAS score
of $80.18$. (Thus losing about $15$ percentage points when going
through a machine-generation phase to obtain UD trees.)

\section{Method}
In essence, our method is to apply, as far as possible, the
tree-to-graph recipes provided by \citet{schuster2016enhanced} to
transform basic UD \emph{trees} into EUD graphs.

\subsection{Procedure} 
To obtain basic UD trees for our system we use the universal
dependency treebanks provided by the shared task organizers. From
these we apply our method on basic UD trees from two sources:

\begin{itemize}
        \item for each language in the test data we use the Stanford
     Biaffine Dependency Parser \citep{dozat2016deep} provided in
     Stanza \citep{qi2020stanza}. We used the stanza model trained on
     the largest treebank of the language.
\item the development and gold trees in the treebanks, \emph{from
      which we have removed the enhanced dependencies}. Indeed, the gold
      data comes with plain UD trees as well.
\end{itemize}

Thus when using the basic UD trees from the Stanford parser we obtain a
baseline for the task, and when using the development/gold trees the
\emph{lower bound} on EUD information contained in UD.

%
Then, we apply a tree-matching procedure against the non-enhanced UD
trees. The procedure locally inserts enhanced edges or deletes unwanted
edges. Additionally (as a special case) the patterns also re-label some edges.

% comment
%Currently out system does not add new nodes for ellipsis

Our system contains several patterns, which are described in the next
subsection. We first apply the patterns that modify
the edge labels with case information. Then we apply all the other
patterns, which add (and sometimes remove) edges from the basic
tree. Here, we only apply the patterns on the (relabeled) input trees
and not on the graph of EUD made by any other pattern. In this sense,
one can say that patterns are applied in parallel.
%
Once this is done, we convert the result back to the
(enhanced) CONLLU format.

\subsection{Patterns}

Perhaps surprisingly, the patterns that we need to recognize are
simple, involving only three nodes. The two patterns to recognize
are shown in \cref{fig:patterns}. Essentially, we need to match
on three connected nodes.  We need to identify two types of
patterns. First, two arcs forming a two-step path
(\cref{fig:pat-nsubj-conj,fig:pat-obl,fig:pat-rel}). We refer to
this style pattern as ``Type 1''.  Second, with two arcs pointing
away from a central node (\cref{fig:pat-aux-conj,fig:pat-xcomp}),
referred to as ``Type 2''.  In both cases, we have additional
constraints on the (edge) labels. Together, the constraints on
graph topology and labels form patterns that we can recognize
and transform.  The exhaustive list of patterns and
transformations follows.

\begin{enumerate}
\item Type 1 pattern, with a relation label, which can be any of
  ``nsubj'',``obj'', ``amod'', ``advcl'',``obl'', ``mark'', ``nmod'', followed by a
  ``conj'' label. (\cref{fig:pat-nsubj-conj}.) In this case we add an
  edge with the relation label to the other conjunct.
  %
  A full dependency tree containing this pattern can be found in \cref{fig:paul-and-mary}.
  \label{item:paul-and-mary}
\item Type 2 pattern with a a relation label being either ``nsubj'' or
  ``aux'', and a ``conj'' label (\cref{fig:pat-aux-conj}). We add a
  relation label to the other conjunct, but only if the
  conjunct is not itself ``nsubj''. Indeed, if it were, then we are
  conjoining two full sentences and then there is no need for an
  enhanced dependency.
  %
  A full dependency tree containing this pattern can be found in \cref{fig:reading-or-watching}.
  \label{item:reading-or-watching}
\item Type 2 pattern with ``xcomp'' and ``nsubj''. Here we add an
  ``nsubj:xsubj'' edge (\cref{fig:pat-xcomp}).
\item Type 1 pattern, with ``acl:relcl'' followed by a relation label
  which can be either ``nsubj'',``obj'',``obl'',
  ``advmod'' (\cref{fig:pat-rel}). The target node should also be a
  \emph{relative} pronoun, ie. it POS is ``PRON'' and its XPOS either
  ``WP'' (who, whom) or ``WDT'' (that, which). Indeed, this pattern is
  also found with other type of pronouns, but then it does not
  correspond to a relative clause.  In this case we add a ``ref'' edge to
  the pronoun and a (reverse) relation edge between the first and
  second node. The original relation edge is deleted. A full dependency
  tree containing this pattern can be seen in \cref{fig:aclrecl}.
  \item Type 1 pattern, with a conjunction followed by a case
   marking (\cref{fig:pat-obl}). Exhaustively, the type of labels
   are ``case'' followed by ``obl'' or ``nmod''; ``cc'' followed by
   ``conj'', ``mark'' followed by ``advcl'' or ``acl''. In this case we
   enhance the label with the lemma of the target node. This pattern
   can be seen in \cref{fig:rel-clause}.
\end{enumerate}

\begin{figure}[h]
    \centering
    \begin{dependency}
    \begin{deptext}[column sep=1em]
    Paul \& and \& Mary \& eat \& . \\
    \end{deptext}
    \depedge{1}{3}{conj:and}
    \depedge{3}{2}{cc}
    \depedge{4}{1}{nsubj}
    \deproot{4}{root}
    \depedge{4}{5}{punct}
    \end{dependency}
    \caption{Example sentence for pattern shown in \cref{fig:pat-nsubj-conj}}
    \label{fig:paul-and-mary}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{dependency}
    \begin{deptext}[column sep=0.1cm]
    She \& was \& reading \& or \& watching \& a \& movie  \\
    \end{deptext}
    \depedge{3}{1}{nsubj}
    \depedge[edge below]{5}{1}{nsubj}
    %\depedge{3}{7}{obj}
    \depedge{5}{7}{obj}
    \deproot{3}{root}
    \depedge{3}{2}{aux}
    \depedge[edge below]{5}{2}{aux}
    \depedge{3}{5}{conj:or}
    \depedge{5}{4}{cc}
    \depedge{7}{6}{det}
    \end{dependency}
    \caption{Example sentence for pattern shown in \cref{fig:pat-aux-conj}}
    \label{fig:reading-or-watching}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{dependency}
    \begin{deptext}[column sep=1em]
    John \& came \& from \& Paris  \\
    \end{deptext}
    \depedge{2}{1}{nsubj}
    \deproot{2}{root}
    \depedge{2}{4}{obl:from}
    \depedge{4}{3}{case}
  \end{dependency}
  \caption{Example sentence for pattern shown in \cref{fig:pat-obl}}
  \label{fig:john-paris}
\end{figure}




\begin{figure}[h]
    \centering
    \begin{dependency}
    \begin{deptext}[column sep=0.1cm]
    Great \& Service \& and \& hairstyles \& that \& last  \\
    \end{deptext}
    \depedge{2}{1}{amod}
    \deproot{2}{root}
    \depedge{4}{3}{cc}
    \depedge{2}{4}{conj:and}
    \depedge{6}{4}{nsubj}
    \depedge{4}{5}{ref}
    \depedge[edge height=1.5cm]{4}{6}{acl:relcl}
    \end{dependency}
    \caption{Example sentence for pattern shown in \cref{fig:pat-rel}}
    \label{fig:aclrecl}
\end{figure}
    

\tikzstyle{word}=[] % [ellipse,draw=blue!50,fill=blue!20,thick]
\tikzstyle{deplabel}=[text=blue]%[rectangle,draw=blue!50,thick]
\tikzstyle{newedge}=[very thick]
\begin{figure}[ht!]
\begin{subfigure}{\columnwidth}
  \centering
  \begin{tikzpicture}[inner sep=1mm]
    \node[word] (eat) {Eat};
    \node[word] (paul) [right=of eat] {Paul};
    \node[word] (mary) [right=of paul] {Mary};
    \draw[->] (eat) -- node[deplabel,above] {nsubj} (paul);
    \draw[->] (paul) -- node[deplabel,above] {conj} (mary);
    \path (eat) edge[->,newedge,bend right]  node[deplabel,below] {nsubj} (mary);
  \end{tikzpicture}
  \caption{Relation pointing to the conjuncts}
  \label{fig:pat-nsubj-conj}
\end{subfigure}

\begin{subfigure}{\columnwidth}
  \centering
\begin{tikzpicture}[inner sep=1mm]
  \node[word] (was) {was};
  \node[word] (read) [right=of was] {read};
  \node[word] (watch) [right=of read] {watch};
  \draw[->] (read) -- node[deplabel,above] {aux} (was);
  \draw[->] (read) -- node[deplabel,above] {conj} (watch);
  \path (watch) edge[->,newedge,bend left]  node[deplabel,below] {aux} (was);
\end{tikzpicture}

  \caption{Relation pointing away from the conjuncts.}
  \label{fig:pat-aux-conj}
\end{subfigure}

\begin{subfigure}{\columnwidth}
  \centering
\begin{tikzpicture}[inner sep=1mm]
  \node[word] (house) {house};
  \node[word] (look) [right=of house] {look};
  \node[word] (new) [right=of look] {new};
  \draw[->] (look) -- node[deplabel,above] {nsubj} (house);
  \draw[->] (look) -- node[deplabel,above] {xcomp} (new);
  \path (new) edge[->,newedge,bend left]  node[deplabel,below] {xsubj:nsubj} (house);
\end{tikzpicture}

  \caption{Xcomp special case}
  \label{fig:pat-xcomp}
\end{subfigure}

\begin{subfigure}{\columnwidth}
  \centering
\begin{tikzpicture}[inner sep=1mm]
  \node[word] (from) {from};
  \node[word] (paris) [right=of from] {Paris};
  \node[word] (come) [right=of paris] {come};
  \draw[->] (paris) -- node[deplabel,below] {case} (from);
  \draw[->] (come) -- node[deplabel,below] (lab) {obl} (paris);
  %\node at (lab.south) {\textbf{obl:from}};
  \node[word] (obl2) [below,xshift=3cm, yshift=-0.4cm] {\textbf{obl:from}};
\end{tikzpicture}
  \caption{Label taken from other word (lemma)}
  \label{fig:pat-obl}
\end{subfigure}

\begin{subfigure}{\columnwidth}
  \centering
  \begin{tikzpicture}[inner sep=1mm]
    \node[word] (boy) {boy};
    \node[word] (live) [right=of boy] {live};
    \node[word] (who) [right=of live] {who};
    \draw[->] (boy) -- node[deplabel,above] {acl:recl} (live);
    \path  (live) edge[->,newedge,bend right=60] node[deplabel,above] {nsubj} (boy);
    \draw[->] (live) -- node[deplabel,above] {nsubj} (who);
    \path (boy) edge[->,newedge,bend right]  node[below,deplabel] {ref} (who);
  \end{tikzpicture}
  \caption{Relative clause}
  \label{fig:pat-rel}
\end{subfigure}

\caption{Implemented transformation patterns. Added elements are shown in bold.}
  \label{fig:patterns}
\end{figure}

\begin{figure}
\begin{subfigure}{\columnwidth}
  \centering
  \begin{tikzpicture}[inner sep=1mm]
    \node[word] (eat) {Eat};
    \node[word] (paul) [right=of eat] {Paul};
    \node[word] (mary) [right=of paul] {Mary};
    \draw[->] (eat) -- node[above] {nsubj} (paul);
    \draw[->] (mary) -- node[above] {conj} (paul);
    \path (eat) edge[->,newedge,bend right]  node[deplabel,below] {nsubj} (mary);
  \end{tikzpicture}
  \caption{Source pattern not representable in UD format}
  \label{fig:pat-nsubj-conj-bad}
\end{subfigure}

\begin{subfigure}{\columnwidth}
  \centering
\begin{tikzpicture}[inner sep=1mm]
  \node[word] (was) {was};
  \node[word] (read) [right=of was] {read};
  \node[word] (watch) [right=of read] {watch};
  \draw[->] (read) -- node[deplabel,above] {aux} (was);
  \draw[->] (watch) -- node[deplabel,above] {conj} (read);
  \path (watch) edge[->,newedge,bend left]  node[deplabel,below] {aux} (was);
\end{tikzpicture}

  \caption{Transformation pattern leading to a loss in performance}
  \label{fig:pat-aux-conj-bad}
\end{subfigure}
\caption{Transformation patterns \textbf{not} implemented.}
\end{figure}

% advcl:mod example
\begin{figure*}[ht]
    \centering
    \begin{dependency}
    \begin{deptext}[column sep=0.1cm]
    One \& answer \& is \& that \& the \& Pentagon \& prevented \& the \& State \& Department \& from \& running \& the \& CPA  \\
    \end{deptext}
    \depedge{2}{1}{nummod}
    \depedge{3}{2}{nsubj}
    \depedge{7}{4}{mark}
    \depedge{6}{5}{det}
    \depedge{7}{6}{nsubj}
    \depedge{3}{7}{ccomp}
    \depedge{10}{8}{det}
    \depedge{10}{9}{compound}
    \depedge{7}{10}{obj}
    \depedge{12}{11}{mark}
    \depedge{7}{12}{advcl:from}
    \depedge{14}{13}{det}
    \depedge{12}{14}{obj}
    \end{dependency}
    \caption{Example sentence for pattern shown in \cref{fig:pat-obl}}
    \label{fig:rel-clause}
\end{figure*}


% \begin{figure}[h]
%     \centering
%     \begin{dependency}
%     \begin{deptext}[column sep=0.5cm]
%     boy \& live \& who  \\
%     \end{deptext}
%     \depedge{1}{2}{acl:recl}
%     \depedge{2}{3}{nsubj}
%     \depedge[edge below,edge style={thick,blue!60}, edge horizontal padding=-3pt]{1}{3}{ref}
%     \depedge[edge below,edge style={thick,blue!60}]{2}{1}{nsubj}
%     \end{dependency}
%     \caption{alternative pattern design}
%     \label{fig:alt-p}
% \end{figure}

\subsubsection{Other patterns}
For patterns \cref{item:paul-and-mary,item:reading-or-watching},
the direction of conjunction dependency could conceptually be
inverted, yielding two other patterns (shown in
\cref{fig:pat-nsubj-conj-bad,fig:pat-aux-conj-bad}). However, we
have not implemented these patterns in our system. For the first
pattern, the reason is simple: it is not representable as a UD
tree (a node cannot have two heads). For the second pattern,
applying it results in a small loss in performance across the
board, and thus it is best left inactive.  Aditionally, we have
not implemented any pattern for dealing with ellipsis in the
current approach. We plan on addressing ellipsis in future work.

\section{Results}
We present our official results for the shared task in
\cref{tab:test}. The scores are obtained by applying our
tree-to-graph transformation to basic dependency trees generated
by the Stanford Dependency Parser.

\begin{table}[h]
	\centering
	\begin{tabular}{l|rr}
		\textsc{Lang} & \textsc{ELAS} & \textsc{EULAS} \\
		\hline
		ar  & 51.26 & 75.62 \\
		bg  & 84.90 & 87.72\\
		cs  & 67.13 & 83.44 \\
		en  & 82.87 & 83.86 \\
		et  & 60.44 & 79.43 \\
		fi  & 65.96 & 83.30 \\
		fr  & 72.76 & 84.39 \\
		it  & 87.14 & 88.74 \\
		lv  & 66.01 & 80.60 \\
		nl  & 78.93 & 80.20 \\
		lt  & 52.56 & 67.37 \\
		pl  & 71.22 & 86.71 \\
		ru  & 70.37 & 88.02 \\
		sk  & 65.16 & 83.31 \\
		sv  & 71.35 & 73.84 \\
		ta  & 42.15 & 55.32 \\
		uk  & 63.24 & 81.24 \\
		\textbf{Avg.} & \textbf{67.85} & \textbf{80.18} \\
        \textbf{Std.} & \textbf{11.69} & \textbf{8.19} \\
	\end{tabular}
\caption{\label{tab:test} Coarse ELAS and EULAS on the languages in the shared task test data.}
\end{table}

The scoring for unlabeled edges is typically around 80\%. The biggest
outlier being Tamil, at 55.32\%. The scoring for labeled edges is
around 12 points lower, with more variation in scores.

As explained in the introduction, to isolate the accuracy of
the tree-to-graph transformation from the performance of the
underlying UD parser, we also apply it to the human-annotated
dependency trees. We test our approach on both the development and
test data. The results from this experiment are shown in
\cref{tab:gold-data}.

\begin{table}[ht]
	\centering
    \small
	\begin{tabular}{l|rrrr}
        & \multicolumn{2}{c}{\textsc{Dev}} & \multicolumn{2}{c}{\textsc{Test}} \\
		\textsc{Lang} & \textsc{ELAS} & \textsc{EULAS} & \textsc{ELAS} & \textsc{EULAS} \\
		\hline 
		ar & 66.44 & 96.38 & 64.14 & 96.55 \\
		bg & 94.32 & 96.95 & 94.55 & 97.01 \\
		cs & 75.65 & 94.63 & 76.52 & 95.12 \\
		en & 97.57 & 98.53 & 97.64 & 98.65 \\
		et & 73.35 & 93.96 & 72.30 & 95.53 \\
		fi & 74.22 & 95.29 & 74.40 & 95.12 \\
		fr & 83.59 & 97.93 & 88.15 & 99.05 \\
		it & 96.29 & 97.61 & 96.15 & 97.77 \\
		lt & 77.50 & 93.98 & 72.17 & 95.49 \\
		lv & 69.21 & 96.10 & 77.47 & 93.91 \\
		nl & 96.71 & 97.55 & 96.27 & 97.57 \\
		pl & 87.03 & 97.33 & 80.95 & 96.49 \\
		ru & 77.22 & 96.31 & 76.72 & 96.59 \\
		sk & 72.24 & 95.82 & 75.37 & 96.42 \\
		sv & 93.85 & 96.66 & 94.19 & 96.67 \\
		ta & 72.49 & 99.58 & 75.04 & 99.48 \\
		uk & 75.95 & 96.04 & 74.42 & 96.49 \\
		\textbf{Avg.} & \textbf{82.97}  & \textbf{96.53}& \textbf{81.55} & \textbf{96.70} \\
        \textbf{Std.} & \textbf{10.42} & \textbf{1.52} & \textbf{10.24} & \textbf{1.43} \\
	\end{tabular}
	\caption{\label{tab:gold-data}Coarse ELAS and EULAS on the gold trees in the development and test data.}
\end{table} 

The scoring for enhanced edges and unenhanced labels (EULAS) is typically
above 95\%, with little variation. Tamil is no longer an
outlier. The scoring for enhanced edges and labels can be
classified into two categories depending on the language. In one
category the ELAS is nearly as good as the EULAS case (bg, en,
it, nl, sv). Another category scores about 15 points lower (ar,
cs, et, fi, lt, lv, ru, sk, ta, uk). French and Polish scores are
somewhere in-between.

Comparing \cref{tab:test} and \cref{tab:gold-data} suggests that
the performance of the full pipeline is highly dependent on the
underlying parser. In addition to the ELAS and EULAS score being
higher, the standard deviation is also much higher for EULAS,
$8.19$ points in the test versus $1.43$ points on the gold data.

Indeed, we also test the basic UD trees provided by Stanza against
the EUD gold data in \cref{tab:test_stanza}, without applying our
algorithm.

\begin{table}[h]
    \centering
    \tiny
    \begin{tabular}{l|rr|rr|rr}
        & \multicolumn{2}{c}{Official submission} & \multicolumn{2}{c}{Stanza trees} & \multicolumn{2}{c}{Gold trees} \\
        \textsc{Lang} & \textsc{ELAS} & \textsc{EULAS} & \textsc{ELAS} & \textsc{EULAS}& \textsc{ELAS} & \textsc{EULAS}\\
        \hline
        \textbf{Avg.} & \textbf{67.85} & \textbf{80.18} & \textbf{66.54} & \textbf{79.91} & \textbf{81.55} & \textbf{96.70} \\
        %\textbf{Std.} & \textbf{11.69} & \textbf{8.19} \\
    \end{tabular}
    \caption{\label{tab:test_stanza} Coarse ELAS and EULAS using our tree-to-graph algorithm on Stanza trees, Stanza trees only and with gold trees.}
\end{table}

We find that using our algorithm with stanza trees only marginally
increase the performance.
%
Given that we know our trees are effective for gold trees, this
indicate that while having a state-of-the-art performance in
dependency parsing these trees have a significant number of
errors for the paths relevant to EUD. If the UD trees were of
good quality for the parts that are relevant for EUD, we would
see a larger increase in performance based on the potential of
our algorithm. To some extent, this indicate that when developing
a EUD parser, the best approach may be to parse UD and EUD in
parallel, and not sequentially. This may help the parser perform
less UD errors for the parts relevant to EUD.

   
\section{Discussion/Analysis}
    
We can draw the following conclusions from our experiments:

\begin{itemize}
\item
  Our pattern recognizer fails to annotate many enhanced labels
  for languages where case is expressed by morphological features.
  
  This is not surprising: we simply did not implement any label rewriting
  based on such features. (Indeed, the pattern \cref{fig:pat-obl}
  recognizes case based on a preposition rather than a morphological
  feature.) This is a shortcoming which we plan to eliminate in future
  work.

  This shortcoming explains much of the discrepancy between the EULAS
  and the ELAS scores.
  
\item The performance of the system is heavily dependent on the
  quality of the UD parser which is used. In other words, our
  experiments show that, even when using good UD parser, much of the
  errors are imputable to the UD parser rather than to the algorithmic
  rewriting step.

\item Our tree-to-graph transformation works well on the gold trees,
  missing less than 4\% of the edges, when given gold UD trees as input.
  Therefore, Broadly speaking, (H1) is vindicated: UD contains most of
  the information which is necessary to reconstruct EUD graphs.

  We note however that (not enhanced) UD is incapable of expressing
  the difference between the structure of the following two sentences:
  
  \begin{itemize}
    \item[(1)] She was reading or watching a movie. (\cref{fig:reading-or-watching})
    \item[(2)] She was cleaning and eating fruits. (\cref{fig:conj-obj})
  \end{itemize}
  In (1) ``a movie'' is an object of a verb (``watching'') which
  is conjoined with another verb (``reading''), but it applies only
  to a single verb. In (2), we also have a conjunction between
  verbs, but ``fruits'' is the object of both verbs. Yet, the UD
  structure is the same for both sentences. And thus, our algorithm
  cannot recognize the difference between the two trees.  One
  solution is to use EUD, but another solution would be to use
  parse trees, which can make the grouping explicit.

  Figuring out which case applies depends on the semantics and
  pragmatics.

\begin{figure}[h]
    \centering
    \begin{dependency}
    \begin{deptext}[column sep=0.1cm]
    She \& was \& cleaning \& and \& eating \& fruit  \\
    \end{deptext}
    \depedge{3}{1}{nsubj}
    \depedge[edge height=2.3cm]{5}{1}{nsubj}
    \deproot{3}{root}
    \depedge{3}{2}{aux}
    \depedge{5}{4}{cc}
    \depedge{3}{5}{conj:and}
    \depedge[edge below]{3}{6}{obj}
    \depedge[edge below]{5}{6}{obj}
    \end{dependency}
    \caption{``She was cleaning and eating fruit''}
    \label{fig:conj-obj}
\end{figure}
 

\end{itemize}

Even though we have shown that UD contains most information to recover
EUD, EUD annotations do have some additional value in certain
cases. However, because they have a more rich structure (graph
vs. tree), it may be that EUD is more suited as inputs to
machine-learning systems. Our systems can help to test this hypothesis
by inserting it (or not) in the training pipeline of a state-of-the-art EUD
parser. If the EUD parser performs just as well with reconstructed EUD
data compared to human-constructed EUD data, then we would know that
manual EUD annotations are not necessary.  It is likely that we would observe
a middle ground situation, where reconstructed EUD data helps, but
does not supplant human-constructed EUD. In such a situation, our
system can be useful as a bootstrapping tool. (We note however that
excellent EUD parsers are not available just yet; in fact they are the
purpose of the IWPT task which our system is entering.)

In addition to bootstrapping, a run of our system can provide a
baseline for such systems.  Indeed, while performing well on gold
data, our approach is transparent (only 5 patterns are applied) and
efficient, requiring a few seconds to generate enhanced graphs from a
treebank. 

\paragraph{ Future work }
%TODO: in a ddition to the short coming ...

In addition to the shortcoming about case labeling mentioned
above, our system struggles with labels that include multi-word
tokens. For example, a valid adverbial clause modifier is
\textit{so\_that}. Currently our system is not able to identify
these. To incorporate this our model would either need to utilize
statistical or deep learning methods, or look for children with
the ``fixed'' relation.
    
%To
%solve these label enhancements our system would need either a complete
%list of multi-word tokens, or be able to figure them out using
%statistical or deep learning.

Another case that our system is not capable of handling is
ellipsis. The problem of ellipsis is difficult, especially in a
tree-rewriting approach. We decided not to tackle this problem and
instead plan on using a deep learning parser for this task in future work.

Yet our main venue for future work is combining our simple, yet effective
method, with deep learning for the problematic cases that are out of
reach. These cases include ellipsis, multi-word tokens, objects of
conjoined verbs.


% polish: case is expressed both in morphology and syntax (through adpositions)

%Our system will also struggle with multi-word labels such as:
%\textsc{advcl:so-that}. To solve these label enhancements a
%system would need either a complete list of these multi-word
%tokens, or be able to figure them out using statistical or deep
%learning

    
\section*{Acknowledgments}
We would like to thank the reviewers for their helpful
comments.The research reported in this paper was supportedby a
grant from the Swedish Research Council (VR project 2014-39) for
the establishment of the Centre for Linguistic Theory and Studies
in Probability (CLASP) at the University of Gothenburg.

\bibliography{anthology,acl2020}
\bibliographystyle{acl_natbib}


\end{document}

% LocalWords:  IWPT UD EUD ELAS mcclosky schuster as well treebanks
% LocalWords:  algorithmically indo european Uralic treebank Biaffine
% LocalWords:  dozat qi CONLLU nsubj amod advcl obl nmod xcomp xsubj
% LocalWords:  acl relcl advmod ie POS XPOS WDT punct det deplabel rr
% LocalWords:  newedge representable nummod ccomp ar bg et fi lv nl
% LocalWords:  lt ru sk sv uk rrrr Dev parsers Anonymized
